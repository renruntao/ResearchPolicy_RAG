import streamlit as st
import openai
from llama_index.llms.openai import OpenAI
try:
    from llama_index import VectorStoreIndex, ServiceContext, Document, SimpleDirectoryReader
except ImportError:
    from llama_index.core import VectorStoreIndex, ServiceContext, Document, SimpleDirectoryReader
from llama_index.core import Settings
from llama_index.embeddings.openai import OpenAIEmbedding
import os
from dotenv import load_dotenv
import base64
import io
import tempfile
import shutil

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = os.path.join(os.path.dirname(__file__), '..', '.env')
load_dotenv(env_path, override=True)  # æ·»åŠ  override=True ç¡®ä¿è¦†ç›–ä»»ä½•ç°æœ‰çš„ç¯å¢ƒå˜é‡

# è¯­è¨€é…ç½®
TRANSLATIONS = {
    "zh": {
        "page_title": "åŸºé‡‘ç”³è¯·æ”¿ç­–æ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
        "page_icon_text": "ğŸ“š",
        "system_config": "ç³»ç»Ÿé…ç½®",
        "upload_pdf": "ä¸Šä¼ PDFæ–‡æ¡£",
        "chunk_size_text": "è®¾ç½®æ–‡æ¡£åˆ†å—å¤§å°",
        "top_k_text": "è®¾ç½®æ£€ç´¢ç»“æœæ•°é‡(Top K)",
        "uploaded_files": "å·²ä¸Šä¼ çš„æ–‡ä»¶ï¼š",
        "reset_button": "é‡ç½®",
        "welcome_message": "æ‚¨å¥½ï¼è¯·å…ˆä¸Šä¼ PDFæ–‡æ¡£,ç„¶åæˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”ç›¸å…³é—®é¢˜ã€‚",
        "preview_title": "æ”¿ç­–æ–‡ä»¶é¢„è§ˆ",
        "qa_title": "æ™ºèƒ½é—®ç­”",
        "input_placeholder": "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š",
        "submit_button": "æäº¤é—®é¢˜",
        "upload_first": "è¯·å…ˆä¸Šä¼ PDFæ–‡æ¡£",
        "processing_error": "æ–‡æ¡£å¤„ç†å¤±è´¥",
        "latest_answer": "æœ€æ–°å›ç­”",
        "reference_basis": "æ¡è§„ä¾æ®",
        "related_basis": "ç›¸å…³ä¾æ®",
        "source_info": "æ¥æºä¿¡æ¯",
        "file_text": "æ–‡ä»¶",
        "page_text": "é¡µç ",
        "chat_history": "æŸ¥çœ‹èŠå¤©å†å²",
        "question_prefix": "é—®é¢˜",
        "answer_prefix": "å›ç­”",
        "loading_text": "æ­£åœ¨åŠ è½½å¹¶ç´¢å¼•æ–‡æ¡£ï¼Œè¯·ç¨å€™...",
        "reference_answer": "å‚è€ƒç­”æ¡ˆ",
        "policy_source": "æ”¿ç­–æ¥æº",
        "page_number": "æ‰€åœ¨é¡µæ•°",
        "page_prefix": "ç¬¬",
        "page_suffix": "é¡µ",
        "pdf_display_error": "PDFæ˜¾ç¤ºå‡ºé”™",
        "doc_process_error": "å¤„ç†é—®é¢˜æ—¶å‡ºé”™"
    },
    "en": {
        "page_title": "Intelligent Q&A System for Fund Application Policies",
        "page_icon_text": "ğŸ“š",
        "system_config": "System Configuration",
        "upload_pdf": "Upload PDF Documents",
        "chunk_size_text": "Set Document Chunk Size",
        "top_k_text": "Set Number of Retrieved Results (Top K)",
        "uploaded_files": "Uploaded Files:",
        "reset_button": "Reset",
        "welcome_message": "Hello! Please upload PDF documents first, then I can help answer your questions.",
        "preview_title": "Policy Document Preview",
        "qa_title": "Intelligent Q&A",
        "input_placeholder": "Enter your question:",
        "submit_button": "Submit Question",
        "upload_first": "Please upload PDF documents first",
        "processing_error": "Document processing failed",
        "latest_answer": "Latest Answer",
        "reference_basis": "Reference Basis",
        "related_basis": "Related Basis",
        "source_info": "Source Information",
        "file_text": "File",
        "page_text": "Page",
        "chat_history": "View Chat History",
        "question_prefix": "Question",
        "answer_prefix": "Answer",
        "loading_text": "Loading and indexing documents, please wait...",
        "reference_answer": "Reference Answer",
        "policy_source": "Policy Source",
        "page_number": "Page Number",
        "page_prefix": "Page",
        "page_suffix": "",
        "pdf_display_error": "PDF display error",
        "doc_process_error": "Error processing question"
    }
}

# åˆå§‹åŒ–è¯­è¨€è®¾ç½®
if 'language' not in st.session_state:
    st.session_state.language = "zh"

# è·å–ç¿»è¯‘æ–‡æœ¬çš„å‡½æ•°
def get_text(key):
    return TRANSLATIONS[st.session_state.language][key]

# é¡µé¢é…ç½®
st.set_page_config(
    page_title=get_text("page_title"),
    page_icon=get_text("page_icon_text"),
    layout="wide",
    initial_sidebar_state="expanded"
)

# OpenAIé…ç½®
api_key = os.getenv('OPENAI_API_KEY')
api_base = os.getenv('OPENAI_API_BASE')

if not api_key:
    st.error("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    st.stop()

# ç¡®ä¿ç¯å¢ƒå˜é‡è¢«æ­£ç¡®è®¾ç½®
os.environ['OPENAI_API_KEY'] = api_key
os.environ['OPENAI_API_BASE'] = api_base if api_base else "https://api.xty.app/v1"

# åœ¨é¡µé¢é…ç½®åæ·»åŠ ä¸´æ—¶æ–‡ä»¶å¤¹ç®¡ç†
if 'temp_dir' not in st.session_state:
    st.session_state.temp_dir = tempfile.mkdtemp()
    
# æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹çš„å‡½æ•°
def cleanup_temp_dir():
    if 'temp_dir' in st.session_state:
        shutil.rmtree(st.session_state.temp_dir)
        del st.session_state.temp_dir

# PDFæ˜¾ç¤ºå‡½æ•°
def display_pdf(pdf_content, page_number=1):
    base64_pdf = base64.b64encode(pdf_content).decode('utf-8')
    pdf_display = f'''
        <iframe 
            src="data:application/pdf;base64,{base64_pdf}#page={page_number}&view=Fit"
            width="700"
            height="1000"
            type="application/pdf"
            style="border: none; margin: 0; padding: 0;"
        ></iframe>
    '''
    st.markdown(
        """
        <style>
        .stMarkdown {
            padding: 0 !important;
            margin: 0 !important;
        }
        iframe {
            display: block;
        }
        </style>
        """, 
        unsafe_allow_html=True
    )
    st.markdown(pdf_display, unsafe_allow_html=True)

# åŠ è½½å’Œç´¢å¼•æ•°æ®
@st.cache_resource(show_spinner=False)
def load_data(pdf_files=None, chunk_size=1000):
    with st.spinner(get_text("loading_text")):
        try:
            Settings.embed_model = OpenAIEmbedding(
                api_base=os.getenv('OPENAI_API_BASE'),
                api_key=os.getenv('OPENAI_API_KEY')
            )
            
            all_docs = []
            
            if pdf_files:
                # ä¿å­˜æ‰€æœ‰ä¸Šä¼ çš„PDFåˆ°ä¸´æ—¶æ–‡ä»¶å¤¹
                for pdf_file in pdf_files:
                    pdf_path = os.path.join(st.session_state.temp_dir, pdf_file.name)
                    with open(pdf_path, 'wb') as f:
                        f.write(pdf_file.getvalue())
                
                # ä»ä¸´æ—¶æ–‡ä»¶å¤¹è¯»å–æ‰€æœ‰æ–‡æ¡£
                reader = SimpleDirectoryReader(input_dir=st.session_state.temp_dir)
                all_docs = reader.load_data()
            else:
                # ä½¿ç”¨ç›¸å¯¹è·¯å¾„åŠ è½½é»˜è®¤æ–‡æ¡£
                default_docs_path = os.path.join(os.path.dirname(__file__), '..', 'data')
                reader = SimpleDirectoryReader(input_dir=default_docs_path, recursive=True)
                all_docs = reader.load_data()
            
            service_context = ServiceContext.from_defaults(
                llm=OpenAI(
                    model="gpt-3.5-turbo", 
                    temperature=0.7,
                    api_base=os.getenv('OPENAI_API_BASE'),
                    api_key=os.getenv('OPENAI_API_KEY')
                ),
                chunk_size=chunk_size
            )
            index = VectorStoreIndex.from_documents(all_docs, service_context=service_context)
            return index, [file.getvalue() for file in pdf_files] if pdf_files else None
            
        except Exception as e:
            st.error(f"{get_text('processing_error')}: {str(e)}")
            return None, None

# ä¿®æ”¹çŠ¶æ€æ§åˆ¶
if 'current_chat_pdf' not in st.session_state:
    st.session_state.current_chat_pdf = None
if 'is_chatting' not in st.session_state:
    st.session_state.is_chatting = False

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.title(get_text("system_config"))
    
    # æ·»åŠ è¯­è¨€é€‰æ‹©
    selected_language = st.selectbox(
        "Language/è¯­è¨€",
        options=["ä¸­æ–‡", "English"],
        index=0 if st.session_state.language == "zh" else 1
    )
    
    # æ›´æ–°è¯­è¨€è®¾ç½®
    if selected_language == "ä¸­æ–‡" and st.session_state.language != "zh":
        st.session_state.language = "zh"
        st.rerun()
    elif selected_language == "English" and st.session_state.language != "en":
        st.session_state.language = "en"
        st.rerun()
    
    uploaded_files = st.file_uploader(get_text("upload_pdf"), type=['pdf'], accept_multiple_files=True)
    chunk_size = st.slider(get_text("chunk_size_text"), 500, 2000, 1000)
    top_k = st.slider(get_text("top_k_text"), 1, 10, 3)
    
    # æ˜¾ç¤ºå·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
    if uploaded_files:
        st.write(get_text("uploaded_files"))
        for file in uploaded_files:
            st.write(f"- {file.name}")
    
    if st.button(get_text("reset_button")):
        cleanup_temp_dir()
        st.session_state.temp_dir = tempfile.mkdtemp()
        if 'chat_engine' in st.session_state:
            del st.session_state.chat_engine
        st.session_state.is_chatting = False
        st.session_state.current_chat_pdf = None
        st.session_state.messages = [
            {"role": "assistant", "content": get_text("welcome_message")}
        ]
        st.rerun()

# ä¸»ç•Œé¢æ ‡é¢˜
st.title(get_text("page_title"))

# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": get_text("welcome_message")}
    ]

# åˆ†åˆ—å¸ƒå±€
col1, col2 = st.columns([0.45, 0.55])

# PDFæ˜¾ç¤ºéƒ¨åˆ†
with col1:
    st.subheader(get_text("preview_title"))
    
    # åˆå§‹çŠ¶æ€æ˜¾ç¤ºNSFCå›¾ç‰‡
    if not st.session_state.is_chatting:
        image_path = os.path.join(os.path.dirname(__file__), '..', 'images', 'logo.jpg')
        st.image(image_path)
    
    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„PDF
    elif st.session_state.current_chat_pdf:
        try:
            st.session_state.current_chat_pdf.seek(0)
            pdf_content = st.session_state.current_chat_pdf.read()
            display_pdf(pdf_content)
        except Exception as e:
            st.error(f"{get_text('pdf_display_error')}: {str(e)}")
            st.session_state.is_chatting = False
            st.session_state.current_chat_pdf = None
            if hasattr(st.session_state, 'current_response'):
                del st.session_state.current_response
            st.rerun()

# é—®ç­”ç•Œé¢
with col2:
    st.subheader(get_text("qa_title"))
    prompt = st.text_input(get_text("input_placeholder"))
    
    if st.button(get_text("submit_button"), use_container_width=True):
        if not uploaded_files:
            st.error(get_text("upload_first"))
            st.stop()
        
        if "chat_engine" not in st.session_state:
            index, _ = load_data(uploaded_files, chunk_size)
            if index:
                st.session_state.chat_engine = index.as_chat_engine(
                    chat_mode="condense_question", 
                    verbose=True,
                    similarity_top_k=top_k
                )
            else:
                st.error(get_text("processing_error"))
                st.stop()
        
        try:
            # è·å–å›ç­”
            response = st.session_state.chat_engine.chat(prompt)
            
            # æ›´æ–°å½“å‰PDFä¸ºæ£€ç´¢åˆ°çš„ç¬¬ä¸€ä¸ªæ–‡æ¡£
            first_node = response.source_nodes[0].node
            file_name = first_node.metadata.get('file_name')
            for file in uploaded_files:
                if file.name == file_name:
                    st.session_state.current_chat_pdf = file
                    st.session_state.is_chatting = True
                    break
            
            # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.current_response = response
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            # ä¿®æ”¹è¾“å‡ºéƒ¨åˆ†çš„ä»£ç 
            output = f"**{get_text('reference_answer')}:** {response.response}\n"
            text_content = response.source_nodes[0].node.text
            cleaned_text = text_content.replace('\n', ' ')  # å…ˆå¤„ç†æ–‡æœ¬
            output += f"**{get_text('reference_basis')}:** {cleaned_text}\n"
            output += f"**{get_text('policy_source')}:** {response.source_nodes[0].node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')}\n"
            output += f"**{get_text('page_number')}:** {get_text('page_prefix')}{response.source_nodes[0].node.metadata.get('page_label', 'æœªçŸ¥')}{get_text('page_suffix')}"
            
            st.session_state.messages.append({"role": "assistant", "content": output})
            
            # å¼ºåˆ¶é‡æ–°æ¸²æŸ“
            st.rerun()
            
        except Exception as e:
            st.error(f"{get_text('doc_process_error')}: {str(e)}")
    
    # æ˜¾ç¤ºå½“å‰å›ç­”ï¼ˆå¦‚æœæœ‰ï¼‰
    if hasattr(st.session_state, 'current_response'):
        response = st.session_state.current_response
        
        # æ˜¾ç¤ºæœ€æ–°çš„å›ç­”
        st.markdown(f"### {get_text('latest_answer')}")
        st.write(response.response)
        st.markdown("---")
        
        # æ˜¾ç¤ºæ¡è§„ä¾æ®
        st.markdown(f"### {get_text('reference_basis')}")
        for i, source_node in enumerate(response.source_nodes):
            # åœ¨æ˜¾ç¤ºæ¡è§„ä¾æ®éƒ¨åˆ†
            with st.expander(f"{get_text('related_basis')} {i+1}", expanded=(i==0)):
                # æ·»åŠ æ–‡æœ¬å†…å®¹
                text = source_node.node.text
                cleaned_text = text.replace('\n', ' ')  # å…ˆå¤„ç†æ–‡æœ¬
                st.markdown(cleaned_text)
                
                # æ·»åŠ åˆ†éš”çº¿
                st.markdown("---")
                
                # æ·»åŠ æ¥æºä¿¡æ¯ï¼Œä½¿ç”¨æ›´æ¸…æ™°çš„æ ¼å¼
                file_name = source_node.node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                page_label = source_node.node.metadata.get('page_label', 'æœªçŸ¥')
                source_info = f"""
                **{get_text('source_info')}**  
                ğŸ“„ {get_text('file_text')}ï¼š{file_name}  
                ğŸ“‘ {get_text('page_text')}ï¼š{get_text('page_prefix')}{page_label}{get_text('page_suffix')}
                """
                st.markdown(source_info)

# æ˜¾ç¤ºèŠå¤©å†å²
with st.expander(get_text("chat_history"), expanded=False):
    for message in st.session_state.messages:
        if message["role"] == "user":
            st.markdown(f"**{get_text('question_prefix')}ï¼š** {message['content']}")
        else:
            st.markdown(f"**{get_text('answer_prefix')}ï¼š** {message['content']}")
        st.markdown("---")